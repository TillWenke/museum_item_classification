{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from deep_translator import GoogleTranslator\n",
    "import re \n",
    "from math import isnan\n",
    "from xgboost import XGBClassifier\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype={'type': str} prevents being confused with data type for large data sets\n",
    "text = pd.read_csv('data/text.csv', index_col='id', dtype={'type': str})\n",
    "train_text = pd.read_csv('data/train_text.csv', index_col='id', dtype={'type': str})\n",
    "test_text = pd.read_csv('data/test_text.csv', index_col='id', dtype={'type': str})\n",
    "#babbage = pd.read_csv('data/embedded_1k_reviews.csv', index_col='id', dtype={'type': str})\n",
    "#curie = pd.read_csv('data/curie.csv', dtype={'type': str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prep.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = ['name', 'commentary', 'text', 'legend', 'initial_info', 'additional_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[text_features] = data[text_features].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_text(item):\n",
    "    return ' '.join(item[text_features]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_features'] = data.apply(lambda item: collect_text(item),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>commentary</th>\n",
       "      <th>text</th>\n",
       "      <th>legend</th>\n",
       "      <th>initial_info</th>\n",
       "      <th>additional_text</th>\n",
       "      <th>text_features</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232170</th>\n",
       "      <td>Kuno Areng, Bremerhaven Festwoche medal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Festwoche - Breemenhaven</td>\n",
       "      <td>KUTTER ASTARTE -SCHIFFERGILDE BREMENHAVEN E.V.</td>\n",
       "      <td>Kuno Areng, Bremerhaven Festwoche medal    Festwoche - Breemenhaven KUTTER ASTARTE -SCHIFFERGILDE BREMENHAVEN E.V.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251378</th>\n",
       "      <td>Photo-Villem Kapp, photo with dedication to Armilde M, 1937</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Photos from the collection of Villem Kapi and Juhan Aavik\\ndesse, purchased in 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Photo-Villem Kapp, photo with dedication to Armilde M, 1937   Photos from the collection of Villem Kapi and Juhan Aavik\\ndesse, purchased in 2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                name  \\\n",
       "id                                                                     \n",
       "232170                       Kuno Areng, Bremerhaven Festwoche medal   \n",
       "2251378  Photo-Villem Kapp, photo with dedication to Armilde M, 1937   \n",
       "\n",
       "        commentary text  \\\n",
       "id                        \n",
       "232170         NaN  NaN   \n",
       "2251378        NaN  NaN   \n",
       "\n",
       "                                                                                      legend  \\\n",
       "id                                                                                             \n",
       "232170                                                                                   NaN   \n",
       "2251378  Photos from the collection of Villem Kapi and Juhan Aavik\\ndesse, purchased in 2013   \n",
       "\n",
       "                     initial_info  \\\n",
       "id                                  \n",
       "232170   Festwoche - Breemenhaven   \n",
       "2251378                       NaN   \n",
       "\n",
       "                                        additional_text  \\\n",
       "id                                                        \n",
       "232170   KUTTER ASTARTE -SCHIFFERGILDE BREMENHAVEN E.V.   \n",
       "2251378                                             NaN   \n",
       "\n",
       "                                                                                                                                             text_features  \n",
       "id                                                                                                                                                          \n",
       "232170                                  Kuno Areng, Bremerhaven Festwoche medal    Festwoche - Breemenhaven KUTTER ASTARTE -SCHIFFERGILDE BREMENHAVEN E.V.  \n",
       "2251378  Photo-Villem Kapp, photo with dedication to Armilde M, 1937   Photos from the collection of Villem Kapi and Juhan Aavik\\ndesse, purchased in 2013  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['name', 'commentary', 'text', 'legend', 'initial_info', 'additional_text', 'text_features']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prep.copy()\n",
    "with_damages = combined_data_fully_translated.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text_features = data.text_features.replace(float('nan'), ' ',)\n",
    "with_damages.damages = with_damages.damages.replace(float('nan'), ' ',)\n",
    "\n",
    "data.text_features = data.text_features + ' ' + with_damages.damages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prep.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data[['text_features','type','source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.to_csv('data/text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text_features = data.text_features.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.text_features != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/text.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check for type contained in texts  ~ rule based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_text.copy()\n",
    "data,test = train_test_split(df, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize types\n",
    "types = [\n",
    "'sculpture',\\\n",
    "'bag', 'suit', 'doll', 'sheet/linen', 'dish/vessel','jewel', 'tape/ribbon',\\\n",
    "'country',\\\n",
    "'paper','book','magazines', 'album', 'newspaper', 'folder/booklet',\\\n",
    "'invitation',  'calendar',\\\n",
    "'audio recording', 'telegram',\\\n",
    "'packaging', 'crate/box',\\\n",
    "'printed notes', 'small print',\\\n",
    "'seal', 'seal/imprint',\\\n",
    "'letter','letter of honor/honorary address',\\\n",
    "'postcard', 'photo, postcard', 'letter, postcard',\\\n",
    "'manuscript','script, song/vocal music', 'music sheet', 'musical instrument', 'manuscript, musical composition', 'manuscript, sheet music',\\\n",
    "'medal', 'coin', 'label/sign',\\\n",
    "'poster','plan','notes', 'document', 'certificate',\\\n",
    "'graphics', 'drawing', 'design/drawing/sketch','caricature','slide',\\\n",
    "'archaeological find',\\\n",
    "'photo', 'photo negative', 'photographic negative, photographic negative', 'photographic material','digital image'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.text_features = data.text_features.replace(float('nan'), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864 2398\n"
     ]
    }
   ],
   "source": [
    "true_counter = 0\n",
    "false_counter = 0\n",
    "false = []\n",
    "for i, item in data.iterrows():    \n",
    "    local_counter = 0\n",
    "    pred = []\n",
    "    for type in types:            \n",
    "        if type in item.text_features:\n",
    "            local_counter += 1\n",
    "            pred.append(type)\n",
    "    if ('drawing' in item.text_features) or ('sketch' in item.text_features) or ('design' in item.text_features):\n",
    "        pred.append('design/drawing/sketch')\n",
    "        local_counter += 1\n",
    "    if 'negative' in item.text_features:\n",
    "        pred.append('photo negative')\n",
    "        local_counter += 1\n",
    "    if 'seal' in item.text_features:\n",
    "        pred.append('seal/imprint')\n",
    "        local_counter += 1\n",
    "    \n",
    "\n",
    "    if local_counter > 0:\n",
    "        if pred[-1] == item.type:           \n",
    "            true_counter += 1\n",
    "        else:\n",
    "            false_counter += 1\n",
    "            false.append(str(pred)+' '+item.type)\n",
    "    \"\"\"\n",
    "    if local_counter > 1:\n",
    "        multi_counter += 1\n",
    "    if local_counter == 1:\n",
    "        one_counter += 1\n",
    "        if pred[0] == item.type:           \n",
    "            true_counter += 1\n",
    "        else:\n",
    "            #print(pred, item.type) \n",
    "            pass\n",
    "    \"\"\"\n",
    "\n",
    "print(false_counter,true_counter)\n",
    "# from 14900 texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"['photo'] digital image\", 112),\n",
       " (\"['album'] photo\", 90),\n",
       " (\"['photo'] photo negative\", 41),\n",
       " (\"['book'] music sheet\", 22),\n",
       " (\"['paper'] newspaper\", 22),\n",
       " (\"['photo negative'] photo\", 21),\n",
       " (\"['photo'] slide\", 19),\n",
       " (\"['slide', 'photo'] slide\", 16),\n",
       " (\"['document', 'photo'] manuscript, sheet music\", 16),\n",
       " (\"['paper'] photo\", 15)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = Counter(false)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_types = list(data.type.value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in types:\n",
    "    all = ''\n",
    "    for i, item in data.iterrows():        \n",
    "        if item.type == type:\n",
    "            all += item.text_features\n",
    "    freq = Counter(all.split())\n",
    "    #print(type, freq.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try with most frequent words only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_type = {}\n",
    "rem_list = ['The','the','in','of','a','and','on','from','is','at','by','with','to','for','as','an','or','are','this','that','be','which','it','its','was','has','have','had','were','their','they','their','there','these','those','such','such','but','not','no','also','all','any','both','each','either','neither','one','other','another','some','such','what','when','where','which','who','whom','whose','why','will','would','can','could','may','might','must','shall','should','will','would','about','above','across','after','against','along','among','around','before','behind','below','beneath','beside','between','beyond','during','except','for','from','in','inside','into','like','near','of','off','on','onto','out','outside','over','past','since','through','to','toward','under','until','up','upon','with','within','without','and/or','A']\n",
    "for type in types:\n",
    "    all = ''\n",
    "    for i, item in data.iterrows():        \n",
    "        if item.type == type:\n",
    "            all += item.text_features\n",
    "    freq = Counter(all.split())\n",
    "    most = freq.most_common(20)\n",
    "    copy = most.copy()\n",
    "    for i in copy:\n",
    "        if i[0] in rem_list:\n",
    "            most.remove(i)\n",
    "    words_per_type[type] = most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148 1987\n"
     ]
    }
   ],
   "source": [
    "true = 0\n",
    "false = 0\n",
    "for i, item in test.iterrows():\n",
    "    cat = ''\n",
    "    max = 0\n",
    "    for type in types:\n",
    "        counter = 0\n",
    "        for word in words_per_type[type]:\n",
    "            if word[0] in item.text_features:\n",
    "                counter += 1\n",
    "        if counter > max:\n",
    "            max = counter\n",
    "            cat = type\n",
    "    if cat == item.type:\n",
    "        true += 1\n",
    "    else:\n",
    "        false += 1\n",
    "\n",
    "print(true,false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sculpture': [('bust', 2),\n",
       "  ('ActHero', 1),\n",
       "  ('Socialist', 1),\n",
       "  ('Labor', 1),\n",
       "  ('sm.', 1),\n",
       "  ('Konijärvboat', 1),\n",
       "  ('modelboat', 1),\n",
       "  ('modelsculpture,', 1),\n",
       "  ('Kaarel', 1),\n",
       "  ('Karm,', 1),\n",
       "  ('1946,', 1),\n",
       "  ('patinated', 1),\n",
       "  ('Purchased', 1),\n",
       "  ('Tea', 1)],\n",
       " 'bag': [('Estonian', 4),\n",
       "  ('bag', 3),\n",
       "  ('Baltic', 3),\n",
       "  ('Regatta', 3),\n",
       "  ('Tallinn', 3),\n",
       "  ('plastic', 3),\n",
       "  ('handbag', 3),\n",
       "  ('front', 2),\n",
       "  ('attached', 2)],\n",
       " 'suit': [('dress', 15),\n",
       "  ('Estonian', 6),\n",
       "  ('made', 6),\n",
       "  ('Dress', 5),\n",
       "  ('M.', 5)],\n",
       " 'doll': [('-', 16),\n",
       "  ('Puppet', 4),\n",
       "  ('Tõnu', 4),\n",
       "  ('Riho', 4),\n",
       "  ('lemur', 4),\n",
       "  ('light', 4),\n",
       "  ('Mouse', 3),\n",
       "  ('Theater', 3),\n",
       "  ('E.', 3),\n",
       "  ('head', 3),\n",
       "  ('silk', 3),\n",
       "  ('Boy', 2),\n",
       "  ('Estonian', 2),\n",
       "  ('National', 2)],\n",
       " 'sheet/linen': [('white', 7),\n",
       "  ('Made', 6),\n",
       "  ('cotton', 5),\n",
       "  ('fabric,', 5),\n",
       "  ('Tallinn', 5),\n",
       "  ('embroidered', 4),\n",
       "  ('decorated', 4),\n",
       "  ('three', 4),\n",
       "  ('red', 4)],\n",
       " 'dish/vessel': [('handle.', 3),\n",
       "  ('small', 3),\n",
       "  ('Received', 3),\n",
       "  ('sides', 3),\n",
       "  ('round', 3),\n",
       "  ('cup', 3),\n",
       "  ('wooden', 3)],\n",
       " 'jewel': [('Keskvere,', 2),\n",
       "  ('Martna,', 2),\n",
       "  ('Läänemaa', 2),\n",
       "  ('Picked', 2),\n",
       "  ('Keskvere', 2),\n",
       "  ('cemetery,', 2),\n",
       "  ('southeast', 2),\n",
       "  ('side', 2)],\n",
       " 'tape/ribbon': [('ribbon', 3),\n",
       "  ('Song', 2),\n",
       "  ('text', 2),\n",
       "  ('blue', 2),\n",
       "  ('costumes', 2),\n",
       "  ('Miina', 2),\n",
       "  ('chest', 2),\n",
       "  ('NO99', 2),\n",
       "  ('Theater', 2),\n",
       "  ('Tape', 1),\n",
       "  ('XV', 1),\n",
       "  ('General', 1)],\n",
       " 'country': [('portrait', 6),\n",
       "  ('paint', 4),\n",
       "  ('country.', 4),\n",
       "  ('Estonian', 4),\n",
       "  ('used', 4),\n",
       "  ('painting,', 3),\n",
       "  ('Ella', 3),\n",
       "  ('Ilbak', 3),\n",
       "  ('–', 3)],\n",
       " 'paper': [(\"Tamme's\", 2),\n",
       "  ('song', 2),\n",
       "  ('Aino', 1),\n",
       "  ('father', 1),\n",
       "  ('Tõnis', 1),\n",
       "  ('materialsTranslated', 1),\n",
       "  ('textsSheet', 1),\n",
       "  ('music,', 1),\n",
       "  ('Kaljuste,', 1),\n",
       "  ('Heino-Nazarova-Medtner', 1),\n",
       "  ('songs', 1),\n",
       "  ('piano', 1),\n",
       "  ('accompanimentLetter', 1),\n",
       "  ('L.', 1),\n",
       "  ('Roosipuu', 1),\n",
       "  ('Adolf', 1),\n",
       "  ('Vedro:', 1)],\n",
       " 'book': [('Estonian', 91),\n",
       "  ('Tallinn', 58),\n",
       "  ('Theater', 49),\n",
       "  ('Tartu', 41),\n",
       "  ('pages.', 36),\n",
       "  ('book', 34),\n",
       "  ('Autograph', 30),\n",
       "  ('covers.', 29),\n",
       "  ('house', 28),\n",
       "  ('Printed', 26),\n",
       "  ('State', 26)],\n",
       " 'magazines': [('No.', 42),\n",
       "  ('magazine,', 16),\n",
       "  ('no.', 14),\n",
       "  ('magazine', 10),\n",
       "  ('Estonian', 7),\n",
       "  ('Music', 6),\n",
       "  ('Theater', 6),\n",
       "  ('1,', 6),\n",
       "  ('Archive', 5),\n",
       "  ('library', 5),\n",
       "  ('2,', 5),\n",
       "  ('Magazine', 4),\n",
       "  ('3,', 4),\n",
       "  ('4,', 4),\n",
       "  ('7,', 3),\n",
       "  ('Magazine,', 3)],\n",
       " 'album': [('album', 5),\n",
       "  ('planet', 4),\n",
       "  ('materials', 3),\n",
       "  ('Estonian', 3),\n",
       "  ('Georg', 3),\n",
       "  ('minor', 3),\n",
       "  ('C.', 2),\n",
       "  ('photo', 2),\n",
       "  ('Museum.', 2)],\n",
       " 'newspaper': [('-', 9),\n",
       "  ('No.', 6),\n",
       "  ('Estonian', 6),\n",
       "  ('newspaper', 6),\n",
       "  ('Pinna', 5),\n",
       "  ('clippings,', 5),\n",
       "  ('clipping', 5),\n",
       "  ('Karl', 3),\n",
       "  ('Leichter,', 3),\n",
       "  ('P.', 3)],\n",
       " 'folder/booklet': [('materials', 5),\n",
       "  ('theater', 3),\n",
       "  ('arrived', 3),\n",
       "  ('museum', 3),\n",
       "  ('Franz', 2),\n",
       "  ('Erika,', 2),\n",
       "  ('Art', 2),\n",
       "  ('historian,', 2),\n",
       "  ('critic', 2),\n",
       "  ('lecturer', 2),\n",
       "  ('Reet', 2),\n",
       "  ('Neimar.', 2),\n",
       "  ('different', 2),\n",
       "  ('times', 2)],\n",
       " 'invitation': [('Karl', 9),\n",
       "  ('concert', 8),\n",
       "  ('Aino', 6),\n",
       "  ('Silvia', 6),\n",
       "  ('anniversary', 5),\n",
       "  ('invitation', 5),\n",
       "  ('Leichter,', 5),\n",
       "  ('Tamme,', 5),\n",
       "  ('stage', 4),\n",
       "  ('card', 4),\n",
       "  ('Tallinn', 4),\n",
       "  ('\"Estonia\"', 3),\n",
       "  ('Leinus,', 3)],\n",
       " 'calendar': [('house', 10),\n",
       "  ('Printing', 9),\n",
       "  ('1', 8),\n",
       "  ('calendar', 5),\n",
       "  ('\"October\".', 4),\n",
       "  ('covers', 3),\n",
       "  ('G.', 3),\n",
       "  ('German.', 3),\n",
       "  ('Tallinn,', 3),\n",
       "  ('yellowed,', 3),\n",
       "  ('sheet.', 3),\n",
       "  ('A.', 3)],\n",
       " 'audio recording': [('B:', 42),\n",
       "  ('A.', 16),\n",
       "  ('words', 14),\n",
       "  ('1.', 13),\n",
       "  ('E.', 13),\n",
       "  ('2.', 12),\n",
       "  ('3.', 12),\n",
       "  ('R.', 11),\n",
       "  ('V.', 11),\n",
       "  ('H.', 11),\n",
       "  ('Part', 11),\n",
       "  ('G.', 10),\n",
       "  ('B.', 10),\n",
       "  ('I.', 8)],\n",
       " 'telegram': [('congratulations', 10),\n",
       "  ('Karl', 9),\n",
       "  ('N.Goldschmidt', 6),\n",
       "  ('Leinus,', 5),\n",
       "  ('anniversary', 5),\n",
       "  ('Hugo', 4),\n",
       "  ('stage', 4),\n",
       "  ('his', 4),\n",
       "  ('telegram', 4),\n",
       "  ('N.', 4),\n",
       "  ('Goldschmidt', 4),\n",
       "  ('Laur:', 3),\n",
       "  ('wrinkledTelegram,', 3)],\n",
       " 'packaging': [('bulb', 4),\n",
       "  ('made', 4),\n",
       "  ('cardboard', 4),\n",
       "  ('items', 3),\n",
       "  ('box', 3),\n",
       "  ('text', 3),\n",
       "  ('2013', 2),\n",
       "  ('Local', 2),\n",
       "  ('Government', 2),\n",
       "  ('Item', 2)],\n",
       " 'crate/box': [('box', 13),\n",
       "  ('paint', 5),\n",
       "  ('worn', 5),\n",
       "  ('belonged', 3),\n",
       "  ('rust', 3)],\n",
       " 'printed notes': [('Autograph', 6),\n",
       "  ('A.', 5),\n",
       "  ('songs', 3),\n",
       "  ('music', 2),\n",
       "  ('Estonian', 2),\n",
       "  ('mixed', 2),\n",
       "  ('choir', 2),\n",
       "  ('sheet', 2),\n",
       "  ('Melodien-Buch', 1),\n",
       "  ('KasemetsYoung', 1),\n",
       "  ('love', 1),\n",
       "  (\"KasemetsDon't\", 1),\n",
       "  ('marry', 1),\n",
       "  ('boyEMO', 1),\n",
       "  ('collection,', 1),\n",
       "  ('M.Saar', 1),\n",
       "  ('\"Hällilaul\"To', 1)],\n",
       " 'small print': [('Estonian', 20),\n",
       "  ('Theater', 18),\n",
       "  ('Dimensions', 16),\n",
       "  ('Council', 13),\n",
       "  ('Tallinn', 11),\n",
       "  ('Dimensions:', 8),\n",
       "  ('Heino', 7),\n",
       "  ('Waks', 7),\n",
       "  ('Reval', 7)],\n",
       " 'seal': [('handle', 12),\n",
       "  ('Association', 10),\n",
       "  ('.....', 9),\n",
       "  ('Tartu', 8),\n",
       "  ('stamp', 6),\n",
       "  ('seal', 5),\n",
       "  ('All', 5),\n",
       "  ('State', 4),\n",
       "  ('Tallinn', 4),\n",
       "  ('head', 4),\n",
       "  ('House', 4),\n",
       "  ('Owners', 4),\n",
       "  ('little', 4),\n",
       "  ('\"...\"', 4)],\n",
       " 'seal/imprint': [('Estonian', 88),\n",
       "  ('seal', 74),\n",
       "  ('Society', 71),\n",
       "  ('collection', 68),\n",
       "  ('Taught', 67),\n",
       "  ('Polish', 66),\n",
       "  ('All', 55),\n",
       "  ('coat', 46),\n",
       "  ('arms', 46),\n",
       "  ('rightSeal', 31),\n",
       "  ('-', 30),\n",
       "  ('church', 21),\n",
       "  ('CrackedSeal', 20),\n",
       "  ('Church', 20)],\n",
       " 'letter': [('letter', 62),\n",
       "  ('Eller,', 40),\n",
       "  ('Heino,', 39),\n",
       "  ('Karl', 33),\n",
       "  ('New', 27),\n",
       "  ('Aino', 27),\n",
       "  ('Music', 26),\n",
       "  ('materials', 26),\n",
       "  ('Ellu', 25),\n",
       "  ('-', 21),\n",
       "  ('Leichter,', 21),\n",
       "  ('Juhan', 21),\n",
       "  ('theater', 21)],\n",
       " 'letter of honor/honorary address': [('honor,', 4),\n",
       "  ('Kaljuste,', 4),\n",
       "  ('Vello', 4),\n",
       "  ('honor', 3),\n",
       "  ('USSR', 3),\n",
       "  ('collection', 3),\n",
       "  ('honorary', 3),\n",
       "  ('Horre', 3),\n",
       "  ('Estonian', 3),\n",
       "  ('Tartu', 2),\n",
       "  ('Art', 2),\n",
       "  ('Decade', 2)],\n",
       " 'postcard': [('Aino', 21),\n",
       "  ('Karl', 12),\n",
       "  ('Leinus,', 9),\n",
       "  ('-', 7),\n",
       "  ('Lepnurm,', 6),\n",
       "  ('postcard', 6),\n",
       "  ('congratulations', 6),\n",
       "  ('Eller,', 5),\n",
       "  ('Heino,', 5),\n",
       "  ('Hugo', 5),\n",
       "  ('Franz', 5),\n",
       "  ('Leichter,', 5),\n",
       "  ('Erika,', 4)],\n",
       " 'photo, postcard': [('view', 2),\n",
       "  ('PHOTOGR.', 2),\n",
       "  ('PARIKAS', 2),\n",
       "  ('Raimund', 2),\n",
       "  ('views', 2),\n",
       "  ('street', 2),\n",
       "  ('house', 2),\n",
       "  ('building', 2),\n",
       "  ('R.', 2),\n",
       "  ('von', 2),\n",
       "  ('Exterior', 1),\n",
       "  ('Lohusuu', 1),\n",
       "  ('Church.', 1)],\n",
       " 'letter, postcard': [('I', 15),\n",
       "  ('you', 10),\n",
       "  ('letter', 6),\n",
       "  ('Süda,', 5),\n",
       "  ('Peeter', 3)],\n",
       " 'manuscript': [('Estonian', 71),\n",
       "  ('Theater', 57),\n",
       "  ('Society', 28),\n",
       "  ('-', 27),\n",
       "  ('song', 17),\n",
       "  ('materials', 14),\n",
       "  ('piano', 13)],\n",
       " 'script, song/vocal music': [('choir', 63),\n",
       "  ('piano', 41),\n",
       "  ('cappella', 33),\n",
       "  ('voice', 32),\n",
       "  ('male', 30),\n",
       "  ('mixed', 27),\n",
       "  ('accompaniment', 22),\n",
       "  ('song', 20),\n",
       "  ('collection', 20),\n",
       "  ('Mart', 15),\n",
       "  ('female', 15),\n",
       "  ('Saare', 14)],\n",
       " 'music sheet': [('music', 54),\n",
       "  ('sheet', 52),\n",
       "  ('Theater', 45),\n",
       "  ('Music', 45),\n",
       "  ('collection', 43),\n",
       "  ('library', 36),\n",
       "  ('personal', 33),\n",
       "  ('Estonian', 30),\n",
       "  ('Archive', 29),\n",
       "  ('Museum', 24),\n",
       "  ('Els', 19),\n",
       "  ('pages.', 19)],\n",
       " 'musical instrument': [('made', 18),\n",
       "  ('instrument', 9),\n",
       "  ('Tallinn', 7),\n",
       "  ('black', 6),\n",
       "  ('box', 6)],\n",
       " 'manuscript, musical composition': [('piano', 9),\n",
       "  ('violin', 8),\n",
       "  ('documentary', 6),\n",
       "  ('-', 5),\n",
       "  ('cello', 4),\n",
       "  ('D', 4),\n",
       "  ('No.', 3),\n",
       "  ('play', 3),\n",
       "  ('choir', 3),\n",
       "  ('Estonian', 3),\n",
       "  ('I', 3),\n",
       "  ('major', 2),\n",
       "  ('wife', 2)],\n",
       " 'manuscript, sheet music': [('Horre', 36),\n",
       "  (\"Zeiger's\", 24),\n",
       "  ('music', 23),\n",
       "  ('documents,', 12),\n",
       "  ('photos,', 12),\n",
       "  ('scores', 12),\n",
       "  ('collected', 12),\n",
       "  ('part', 12),\n",
       "  ('popular', 12),\n",
       "  ('exhibition', 12),\n",
       "  ('project.', 12),\n",
       "  ('material', 12),\n",
       "  ('musical', 12),\n",
       "  ('historical', 12)],\n",
       " 'medal': [('his', 19),\n",
       "  ('Estonian', 13),\n",
       "  ('collection', 10),\n",
       "  ('museum', 10),\n",
       "  ('Estonia', 9),\n",
       "  ('Triumph', 8),\n",
       "  ('On', 8)],\n",
       " 'coin': [('denarius', 69),\n",
       "  ('metal', 66),\n",
       "  ('coin', 59),\n",
       "  ('treasures', 56),\n",
       "  ('hoards', 48),\n",
       "  ('containera', 39),\n",
       "  ('penny', 32),\n",
       "  ('Dbg.', 26),\n",
       "  ('S.', 18),\n",
       "  ('Hatz', 15),\n",
       "  ('act', 14),\n",
       "  ('found', 12),\n",
       "  ('Heritage', 10)],\n",
       " 'label/sign': [('-', 12),\n",
       "  ('Laine', 12),\n",
       "  ('song', 11),\n",
       "  ('dance', 7),\n",
       "  ('sign', 6),\n",
       "  ('Tallinn', 6)],\n",
       " 'poster': [('Estonian', 156),\n",
       "  ('concert', 53),\n",
       "  ('Russian', 50),\n",
       "  ('Drama', 46),\n",
       "  ('theater', 44),\n",
       "  ('In', 41),\n",
       "  ('poster', 38),\n",
       "  ('Estonia', 38),\n",
       "  ('Theater,', 36),\n",
       "  ('music', 34)],\n",
       " 'plan': [('concert', 61),\n",
       "  ('Symphony', 51),\n",
       "  ('-', 42),\n",
       "  ('concert,', 39),\n",
       "  ('Franz', 39),\n",
       "  ('Erika,', 39),\n",
       "  ('Estonia', 35),\n",
       "  ('Visnapuu,', 34),\n",
       "  ('materials', 33),\n",
       "  ('Karl', 33),\n",
       "  ('Eduard', 33),\n",
       "  ('M', 30),\n",
       "  ('collected', 30),\n",
       "  ('Leichter,', 28)],\n",
       " 'notes': [('Franz', 2),\n",
       "  ('Erika,', 2),\n",
       "  ('notes', 2),\n",
       "  ('Rewritten', 1),\n",
       "  ('Juhan', 1),\n",
       "  ('SimmiNotes,', 1),\n",
       "  ('Exam', 1),\n",
       "  ('NotesManuscript,', 1),\n",
       "  ('Eduard', 1),\n",
       "  ('Visnapuu,', 1),\n",
       "  ('Self-improvement', 1),\n",
       "  (\"notesDirector's\", 1),\n",
       "  ('playPeeter', 1),\n",
       "  (\"Piilpärg's\", 1)],\n",
       " 'document': [('Estonian', 19),\n",
       "  ('Folk', 14),\n",
       "  ('A.', 14),\n",
       "  ('-', 13),\n",
       "  ('Theater', 12),\n",
       "  ('instrumentalists', 12),\n",
       "  ('Estonia', 12),\n",
       "  ('New', 11)],\n",
       " 'certificate': [('1972-1976', 1),\n",
       "  ('1972-1976;', 1),\n",
       "  ('hard', 1),\n",
       "  ('gray', 1),\n",
       "  ('covers,', 1),\n",
       "  ('logo', 1),\n",
       "  ('name', 1),\n",
       "  ('golden', 1),\n",
       "  ('letters', 1),\n",
       "  ('topMedal', 1),\n",
       "  ('certificate:', 1),\n",
       "  ('Einar', 1),\n",
       "  ('Koppel,', 1),\n",
       "  ('Lenin', 1),\n",
       "  ('10014', 1),\n",
       "  ('Dec', 1)],\n",
       " 'graphics': [('yellowed,', 32),\n",
       "  ('moisture', 21),\n",
       "  ('portrait', 17),\n",
       "  ('slightly', 17),\n",
       "  ('edges', 14),\n",
       "  ('soiled,', 13),\n",
       "  ('corners', 13),\n",
       "  ('pasted', 9),\n",
       "  ('yellowedgraphics,', 8),\n",
       "  ('right', 8)],\n",
       " 'drawing': [('yellowed,', 6),\n",
       "  ('Ots,', 5),\n",
       "  ('Georg', 5),\n",
       "  ('personal', 5),\n",
       "  ('collection;', 5),\n",
       "  ('handed', 5),\n",
       "  ('Ilona', 5),\n",
       "  ('Otsa', 5),\n",
       "  ('view', 4),\n",
       "  ('city', 4),\n",
       "  ('portrait', 3)],\n",
       " 'design/drawing/sketch': [('design', 458),\n",
       "  ('production', 203),\n",
       "  ('play', 181),\n",
       "  ('Theater', 178),\n",
       "  ('character', 153),\n",
       "  ('Vanemuine', 120),\n",
       "  ('theater', 96),\n",
       "  ('actor', 83),\n",
       "  ('Estonian', 76),\n",
       "  ('costume', 70),\n",
       "  ('Drama', 61)],\n",
       " 'caricature': [('theater', 9),\n",
       "  ('caricature', 6),\n",
       "  ('competition', 6),\n",
       "  ('pen', 4),\n",
       "  ('-', 4),\n",
       "  ('month', 3),\n",
       "  ('work', 3),\n",
       "  ('black', 3)],\n",
       " 'slide': [('photo', 34),\n",
       "  ('ETMM', 27),\n",
       "  ('lab.', 27),\n",
       "  ('Henno', 24),\n",
       "  ('slide.', 23),\n",
       "  ('Saarne', 23),\n",
       "  ('Color', 22),\n",
       "  ('slide', 19),\n",
       "  ('photo.', 18),\n",
       "  ('Work', 17),\n",
       "  ('General', 14),\n",
       "  ('-', 13),\n",
       "  ('party.', 12),\n",
       "  ('Song', 11)],\n",
       " 'archaeological find': [('Mandel', 343),\n",
       "  ('excavations', 264),\n",
       "  ('Obtained', 249),\n",
       "  ('led', 245),\n",
       "  ('Mati', 225),\n",
       "  ('M.', 149),\n",
       "  ('1983-1990.', 136),\n",
       "  ('(II', 136),\n",
       "  ('earthenware', 123),\n",
       "  ('Complained', 90),\n",
       "  ('Died', 76)],\n",
       " 'photo': [('photo', 300),\n",
       "  ('-', 231),\n",
       "  ('handed', 230),\n",
       "  ('album', 220),\n",
       "  ('Museum', 213),\n",
       "  ('photos', 197),\n",
       "  ('collection', 176)],\n",
       " 'photo negative': [('-', 463),\n",
       "  ('negatives', 154),\n",
       "  ('Estonia,', 153),\n",
       "  ('Estonian', 136),\n",
       "  ('Teater', 112),\n",
       "  ('Museum', 106),\n",
       "  ('picture:', 101),\n",
       "  ('History', 93),\n",
       "  ('parts:', 86),\n",
       "  ('collection', 84)],\n",
       " 'photographic negative, photographic negative': [('Photo', 1),\n",
       "  ('negative', 1),\n",
       "  ('B.', 1),\n",
       "  ('Pepper', 1)],\n",
       " 'photographic material': [('-', 8),\n",
       "  ('\"The', 7),\n",
       "  ('Estonia,', 7),\n",
       "  ('Photo', 4),\n",
       "  ('emulsion', 4),\n",
       "  ('glass', 4),\n",
       "  ('\"Dolly\"', 4),\n",
       "  ('(Hugo', 4),\n",
       "  ('Hirsch).', 4),\n",
       "  ('theater', 4),\n",
       "  ('\"Estonia\",', 4),\n",
       "  ('Bartered', 3),\n",
       "  ('Theater', 3),\n",
       "  ('plate', 3),\n",
       "  ('Drama', 3),\n",
       "  ('\"Estonia\".', 2),\n",
       "  ('Karl', 2)],\n",
       " 'digital image': [('Rakvere', 79),\n",
       "  ('photos', 71),\n",
       "  ('productions', 70),\n",
       "  ('Theater', 64),\n",
       "  ('-', 59),\n",
       "  ('theater', 44),\n",
       "  ('Photographer', 41),\n",
       "  ('Premiere', 40),\n",
       "  ('Ugala', 36),\n",
       "  ('2015', 32),\n",
       "  ('production,', 29),\n",
       "  ('2006-2017', 29),\n",
       "  ('hall,', 29),\n",
       "  ('Estonian', 26)]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_per_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpt3 embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "52\n",
      "97\n",
      "148\n",
      "185\n",
      "228\n",
      "277\n",
      "323\n",
      "359\n",
      "398\n",
      "441\n",
      "477\n",
      "512\n",
      "554\n",
      "597\n",
      "636\n",
      "675\n",
      "712\n",
      "747\n",
      "786\n",
      "831\n",
      "868\n",
      "911\n",
      "951\n",
      "997\n",
      "1046\n",
      "1097\n",
      "1137\n",
      "1172\n",
      "1209\n",
      "1254\n",
      "1293\n",
      "1327\n",
      "1362\n",
      "1409\n",
      "1448\n",
      "1483\n",
      "1530\n",
      "1573\n",
      "1608\n",
      "1639\n",
      "1681\n",
      "1725\n",
      "1769\n",
      "1805\n",
      "1857\n",
      "1905\n",
      "1946\n",
      "2000\n",
      "2042\n",
      "2081\n",
      "2124\n",
      "2158\n",
      "2193\n",
      "2225\n",
      "2264\n",
      "2304\n",
      "2355\n",
      "2394\n",
      "2436\n",
      "2479\n",
      "2513\n",
      "2555\n",
      "2596\n",
      "2629\n",
      "2668\n",
      "2709\n",
      "2759\n",
      "2809\n",
      "2839\n",
      "2884\n",
      "2927\n",
      "2963\n",
      "3010\n",
      "3054\n",
      "3103\n",
      "3152\n",
      "3198\n",
      "3246\n",
      "3296\n",
      "3338\n",
      "3379\n",
      "3425\n",
      "3466\n",
      "3512\n",
      "3548\n",
      "3591\n",
      "3640\n",
      "3681\n",
      "3720\n",
      "3766\n",
      "3821\n",
      "3862\n",
      "3904\n",
      "3948\n",
      "3989\n",
      "4034\n",
      "4078\n",
      "4117\n",
      "4147\n",
      "4178\n",
      "4212\n",
      "4250\n",
      "4280\n",
      "4324\n",
      "4370\n",
      "4406\n",
      "4436\n",
      "4484\n",
      "4518\n",
      "4559\n",
      "4595\n",
      "4648\n",
      "4683\n",
      "4722\n",
      "4754\n",
      "4805\n",
      "4852\n",
      "4900\n",
      "4950\n",
      "4988\n",
      "5036\n",
      "5072\n",
      "5119\n",
      "5157\n",
      "5196\n",
      "5224\n",
      "5257\n",
      "5298\n",
      "5350\n",
      "5394\n",
      "5427\n",
      "5468\n",
      "5513\n",
      "5554\n",
      "5592\n",
      "5640\n",
      "5679\n",
      "5718\n",
      "5754\n",
      "5801\n",
      "5850\n",
      "5898\n",
      "5934\n",
      "5975\n",
      "6022\n",
      "6054\n",
      "6106\n",
      "6142\n",
      "6177\n",
      "6220\n",
      "6260\n",
      "6299\n",
      "6326\n",
      "6378\n",
      "6423\n",
      "6459\n",
      "6506\n",
      "6549\n",
      "6592\n",
      "6637\n",
      "6677\n",
      "6725\n",
      "6764\n",
      "6807\n",
      "6860\n",
      "6900\n",
      "6952\n",
      "7001\n",
      "7035\n",
      "7076\n",
      "7125\n",
      "7166\n",
      "7213\n",
      "7264\n",
      "7309\n",
      "7354\n",
      "7404\n",
      "7449\n",
      "7483\n",
      "7524\n",
      "7559\n",
      "7593\n",
      "7633\n",
      "7682\n",
      "7736\n",
      "7775\n",
      "7814\n",
      "7862\n",
      "7907\n",
      "7956\n",
      "8002\n",
      "8042\n",
      "8073\n",
      "8111\n",
      "8158\n",
      "8204\n",
      "8245\n",
      "8272\n",
      "8314\n",
      "8352\n",
      "8397\n",
      "8433\n",
      "8470\n",
      "8516\n",
      "8565\n",
      "8600\n",
      "8629\n",
      "8664\n",
      "8712\n",
      "8757\n",
      "8783\n",
      "8833\n",
      "8879\n",
      "8927\n",
      "8965\n",
      "9005\n",
      "9041\n",
      "9089\n",
      "9137\n",
      "9174\n",
      "9224\n",
      "9265\n",
      "9304\n",
      "9339\n",
      "9376\n",
      "9416\n",
      "9452\n",
      "9488\n",
      "9518\n",
      "9556\n",
      "9605\n",
      "9645\n",
      "9683\n",
      "9723\n",
      "9756\n",
      "9803\n",
      "9845\n",
      "9886\n",
      "9925\n",
      "9968\n",
      "10004\n",
      "10044\n",
      "10088\n",
      "10120\n",
      "10162\n",
      "10205\n",
      "10244\n",
      "10284\n",
      "10331\n",
      "10374\n",
      "10413\n",
      "10446\n",
      "10485\n",
      "10522\n",
      "10564\n",
      "10604\n",
      "10643\n",
      "10689\n",
      "10739\n",
      "10777\n",
      "10820\n",
      "10856\n",
      "10893\n",
      "10935\n",
      "10967\n",
      "11008\n",
      "11049\n",
      "11101\n",
      "11145\n",
      "11196\n",
      "11228\n",
      "11259\n",
      "11308\n",
      "11351\n",
      "11388\n",
      "11430\n",
      "11462\n",
      "11503\n",
      "11546\n",
      "11569\n",
      "11612\n",
      "11656\n",
      "11703\n",
      "11743\n",
      "11780\n",
      "11826\n",
      "11864\n",
      "11910\n",
      "11960\n",
      "12002\n",
      "12034\n",
      "12082\n",
      "12111\n",
      "12146\n",
      "12198\n",
      "12240\n",
      "12284\n",
      "12328\n",
      "12380\n",
      "12421\n",
      "12461\n",
      "12489\n",
      "12532\n",
      "12576\n",
      "12621\n",
      "12663\n",
      "12704\n",
      "12744\n",
      "12791\n",
      "12833\n",
      "12867\n",
      "12919\n",
      "12964\n",
      "13003\n",
      "13047\n",
      "13083\n",
      "13106\n",
      "13140\n",
      "13179\n",
      "13203\n",
      "13237\n",
      "13267\n",
      "13304\n",
      "13335\n",
      "13376\n",
      "13412\n",
      "13445\n",
      "13489\n",
      "13522\n",
      "13565\n",
      "13597\n",
      "13636\n",
      "13692\n",
      "13733\n",
      "13772\n",
      "13814\n",
      "13865\n",
      "13910\n",
      "13951\n",
      "13993\n",
      "14035\n",
      "14073\n",
      "14111\n",
      "14140\n",
      "14189\n",
      "14229\n",
      "14270\n",
      "14316\n",
      "14366\n",
      "14409\n",
      "14443\n",
      "14482\n",
      "14520\n",
      "14554\n",
      "14596\n",
      "14639\n",
      "14674\n",
      "14710\n",
      "14755\n",
      "14793\n",
      "14826\n",
      "14875\n",
      "14921\n",
      "14960\n",
      "14994\n",
      "15046\n",
      "15072\n",
      "15122\n",
      "15167\n",
      "15218\n",
      "15262\n",
      "15316\n",
      "15353\n",
      "15401\n",
      "15433\n",
      "15482\n",
      "15521\n",
      "15551\n",
      "15598\n",
      "15642\n",
      "15685\n",
      "15720\n",
      "15761\n",
      "15806\n",
      "15847\n",
      "15889\n",
      "15926\n",
      "15973\n",
      "16010\n",
      "16047\n",
      "16088\n",
      "16132\n",
      "16173\n",
      "16213\n",
      "16264\n",
      "16296\n",
      "16342\n",
      "16382\n",
      "16430\n",
      "16475\n",
      "16516\n",
      "16559\n",
      "16600\n",
      "16637\n",
      "16685\n",
      "16717\n",
      "16756\n",
      "16799\n",
      "16840\n",
      "16891\n",
      "16928\n",
      "16965\n",
      "17016\n",
      "17046\n",
      "17093\n",
      "17137\n",
      "17181\n",
      "17212\n",
      "17263\n",
      "17308\n",
      "17343\n",
      "17390\n",
      "17429\n",
      "17468\n",
      "17495\n",
      "17548\n",
      "17590\n",
      "17635\n",
      "17665\n",
      "17702\n",
      "17749\n",
      "17780\n",
      "17823\n",
      "17866\n",
      "17901\n",
      "17941\n",
      "17975\n",
      "18018\n",
      "18058\n",
      "18097\n",
      "18138\n",
      "18180\n",
      "18227\n",
      "18278\n",
      "18320\n",
      "18356\n",
      "18396\n",
      "18434\n",
      "18484\n",
      "18534\n",
      "18579\n",
      "18613\n",
      "18653\n",
      "18685\n",
      "18730\n",
      "18782\n",
      "18827\n",
      "18868\n",
      "18921\n",
      "18965\n",
      "19011\n",
      "19050\n",
      "19085\n",
      "19129\n",
      "19166\n",
      "19207\n",
      "19253\n",
      "19290\n",
      "19339\n",
      "19379\n",
      "19428\n",
      "19466\n",
      "19514\n",
      "19544\n",
      "19583\n",
      "19623\n",
      "19672\n",
      "19717\n",
      "19757\n",
      "19792\n",
      "19834\n",
      "19872\n",
      "19920\n",
      "19958\n",
      "19991\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "openai.api_key = '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'\n",
    "count = 0\n",
    "def get_embedding(text, model=\"text-similarity-davinci-001\"):\n",
    "    global count\n",
    "    count += 1\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    try:\n",
    "        result = openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "    except:\n",
    "        print(count)\n",
    "        time.sleep(60)\n",
    "        result = openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "    return result\n",
    " \n",
    "text['curie_similarity'] = text.text_features.apply(lambda x: get_embedding(x, model='text-similarity-curie-001'))\n",
    "text.to_csv('data/curie.csv', index=False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = curie.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb = df[df.source == 'train']\n",
    "test_emb = df[df.source == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb.to_csv('data/train_curie.csv', index=True)\n",
    "test_emb.to_csv('data/test_curie.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rf on embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_curie.csv', index_col='id', dtype={'type': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['curie_similarity'] = df.curie_similarity.apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    " \n",
    "features = list(df.curie_similarity.values)\n",
    "labels = df.type\n",
    "# at least xgboost cannot deal with string labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(labels)\n",
    "labels = label_encoder.transform(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from deep_translator import GoogleTranslator\n",
    "import re \n",
    "from math import isnan\n",
    "import wandb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "\n",
    "df = pd.read_csv('data/train_curie.csv', index_col='id', dtype={'type': str})\n",
    "df['curie_similarity'] = df.curie_similarity.apply(eval).apply(np.array)\n",
    "print('eval')\n",
    "\n",
    "features = list(df.curie_similarity.values)\n",
    "labels = df.type\n",
    "# at least xgboost cannot deal with string labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(labels)\n",
    "labels = label_encoder.transform(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=0)\n",
    "\n",
    "bst = XGBClassifier(random_state=0)\n",
    "print('run')\n",
    "# fit model\n",
    "bst.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '[0.01085097 0.01198783 0.00867431 ... 0.0049957  0.02543602 0.01269028]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/till/projects/uni/data science/museum_item_classification/nlp.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/till/projects/uni/data%20science/museum_item_classification/nlp.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report, accuracy_score\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/till/projects/uni/data%20science/museum_item_classification/nlp.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m clf \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/till/projects/uni/data%20science/museum_item_classification/nlp.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/till/projects/uni/data%20science/museum_item_classification/nlp.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m preds \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/till/projects/uni/data%20science/museum_item_classification/nlp.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m probas \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict_proba(X_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:331\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 331\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    332\u001b[0m     X, y, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mDTYPE\n\u001b[1;32m    333\u001b[0m )\n\u001b[1;32m    334\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1075\u001b[0m     X,\n\u001b[1;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1077\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1078\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1079\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1080\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1081\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1082\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1083\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1084\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1085\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1086\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '[0.01085097 0.01198783 0.00867431 ... 0.0049957  0.02543602 0.01269028]'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    " \n",
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "probas = clf.predict_proba(X_test)\n",
    "\n",
    "report = classification_report(y_test, preds)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/till/projects/uni/data science/museum_item_classification/nlp.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/till/projects/uni/data%20science/museum_item_classification/nlp.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m clf\u001b[39m.\u001b[39msave_model(\u001b[39m'\u001b[39m\u001b[39mmodels/curie_model.json\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "clf.save_model('models/curie_model.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
