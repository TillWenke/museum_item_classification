{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setup_general import *\n",
    "from prep_helpers import *\n",
    "\n",
    "def get_data(feat_percent_cut, feat_freq_cut):\n",
    "    lang = 'est'\n",
    "    if lang == 'en':\n",
    "        data = combined_data_fully_translated.copy()\n",
    "    if lang == 'est':\n",
    "        data = combined_data.copy()\n",
    "    # Feature specific engineering\n",
    "    ## units - sizes -values\n",
    "    # Finish unit translation/ unification &  values to float\n",
    "    data['value'] = data['value'].apply(lambda x: float(x.replace(',', '.')) if type(x) == str else x)\n",
    "\n",
    "    # unify units\n",
    "    data['unit'] = data['unit'].replace('10 x 15 cm','100 x 150 mm')\n",
    "\n",
    "    # mm to cm\n",
    "    data['value'] = data.apply(lambda item: item['value'] / 10 if item['unit'] == 'mm' else item['value'], axis=1)\n",
    "    data['unit'] = data['unit'].replace('mm','cm')\n",
    "    data['value'] = pd.to_numeric(data['value'])    \n",
    "\n",
    "    data['unit'] = data['unit'].replace(np.nan,'*')\n",
    "    data['parameter'] = data['parameter'].replace(np.nan,'*')\n",
    "    data['unit'] = data['unit'].apply(lambda x: get_squared(x))\n",
    "    # execution order is important\n",
    "    data['value'] = data.apply(lambda item: extract_width_height_from_unit_to_value(item[['unit','value']])[1], axis=1)\n",
    "    data['unit'] = data.apply(lambda item: extract_width_height_from_unit_to_value(item[['unit','value']])[0], axis=1)\n",
    "    data['parameter_and_unit'] = data['parameter'] + ' IN ' + data['unit']\n",
    "\n",
    "    # parameter_and_units as single features with respective values\n",
    "    # parameter_and_unit turned into one hot encoded features\n",
    "    data = pd.get_dummies(data, columns=['parameter_and_unit'], prefix='', prefix_sep='')\n",
    "\n",
    "    #  for all new \"parameter with unit\" columns put the value in the column where a 1 is - others are 0 and remain 0\n",
    "    for column in data.columns:\n",
    "        if ' IN ' in column and '*' not in column:\n",
    "            data[column] = data.apply(lambda item: extract_value(item['value'], item[column]), axis=1)            \n",
    "\n",
    "    for column in data.columns:\n",
    "        # all the parameter with unit columns that contain arrays that are represeted as strings\n",
    "        if (' IN ' in column) and (data[column].dtype == object):\n",
    "            data[column + '_height'] = data.apply(lambda item: extract_height_width(item[column])[0], axis=1)\n",
    "            data[column + '_width'] = data.apply(lambda item: extract_height_width(item[column])[1], axis=1)\n",
    "            pd.to_numeric(data[column + '_height'])\n",
    "            pd.to_numeric(data[column + '_width'])\n",
    "            data = data.drop(column, axis=1)\n",
    "\n",
    "    for column in data.columns:\n",
    "        if (' IN ' in column):\n",
    "            data[column] = data[column].replace(np.nan,0)\n",
    "    \n",
    "\n",
    "    data['country_and_unit'] = data.apply(lambda x: empty_to_nan(x['country_and_unit']), axis=1)\n",
    "    data['technique'] = data['technique'].apply(lambda x: x.strip() if (type(x) == str) else x)\n",
    "    ## country_unit - material - technique - location (splitting for features including multiple information)\n",
    "\n",
    "    \n",
    "\n",
    "    data['city_municipality'] = data.apply(lambda item: extract_city_country(item['country_and_unit'])[0], axis=1)\n",
    "    data['country'] = data.apply(lambda item: extract_city_country(item['country_and_unit'])[1], axis=1)\n",
    "\n",
    "    # material\n",
    "    # to make the following work even for nan values\n",
    "    data['material'] = data['material'].replace(np.nan, 'nan')\n",
    "    # prepare single values to be distinguishable\n",
    "    data['material'] = data['material'].apply(lambda x: x.split('>'))\n",
    "\n",
    "    # https://stackoverflow.com/questions/45312377/how-to-one-hot-encode-from-a-pandas-column-containing-a-list\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    data = data.join(pd.DataFrame(mlb.fit_transform(data.pop('material')),\n",
    "                            columns='material_' + mlb.classes_,\n",
    "                            index=data.index))\n",
    "\n",
    "    # technique\n",
    "    # to make the following work even for nan values\n",
    "    data['technique'] = data['technique'].replace(np.nan, 'nan')\n",
    "\n",
    "    # prepare single values to be distinguishable\n",
    "    data['technique'] = data['technique'].apply(lambda x: x.split('>'))\n",
    "\n",
    "    data = data.join(pd.DataFrame(mlb.fit_transform(data.pop('technique')),\n",
    "                            columns='technique_' + mlb.classes_,\n",
    "                            index=data.index), rsuffix='')\n",
    "\n",
    "    # location\n",
    "    data['location_city'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('linn ' in x) else 0)\n",
    "    data['location_building'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('hoone ' in x) else 0)\n",
    "    data['location_street'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('tÃ¤nav ' in x) else 0)\n",
    "    data['location_country'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('riik ' in x) else 0)\n",
    "    data['location_address'] = data['location'].apply(lambda x: 1 if (type(x) == str) and ('aadress ' in x) else 0)\n",
    "    # start - end (formatting)\n",
    "\n",
    "    data['start'] = data[['name', 'start']].apply(extract_year_from_name, axis=1)    \n",
    "        \n",
    "    #grouping applied to the dataframe\n",
    "    data['startYear'] = data['start'].apply(year_Grouping)\n",
    "    data['startMonth'] = data['start'].apply(month_Grouping)\n",
    "    data['startDay'] = data['start'].apply(day_Grouping)\n",
    "            \n",
    "    data['endYear'] = data['end'].apply(year_Grouping)\n",
    "    data['endMonth'] = data['end'].apply(month_Grouping)\n",
    "    data['endDay'] = data['end'].apply(day_Grouping)\n",
    "\n",
    "    #if there is no start year, but an end year, then the start year is set to the end year\n",
    "    for i in range(1,len(data)):\n",
    "        if data['startYear'].iloc[i] == 0 and data['startDay'].iloc[i] != 0:\n",
    "            data['startYear'].iloc[i] = data['endYear'].iloc[i]\n",
    "\n",
    "\n",
    "    #original columns are dropped as they are no longer needed\n",
    "    data.drop(['start', 'end'], axis=1, inplace=True)\n",
    "    ## event_type (brackets)\n",
    "\n",
    "    data['event_type'] = data['event_type'].apply(strip_brackets)\n",
    "    ## color (grouping)\n",
    "    #Grouping colours by their base colour - to avoid too many extra cloumns when hot encoding -> could always reverse this step\n",
    "    #by using  something like data['color'] = combined_data_translated['color'] ?\n",
    "\n",
    "    #The base colours: red, blue, green, grey, yellow, patterned, orange, brown, white, black , pink\n",
    "    #The most common/distingtive stay unchanged\n",
    "\n",
    "    #apply colour_grouping to the dataset\n",
    "    data['color'] = data['color'].apply(colour_grouping)\n",
    "    ## technique - material - sizes (threshold previously encoded)\n",
    "\n",
    "    # best found combination (local optimum on 500 estimators)\n",
    "    perc = feat_percent_cut/100\n",
    "    threshold_sum = len(data) * perc\n",
    "    min_freq = feat_freq_cut\n",
    "\n",
    "    tech = helpers.col_collection(data, 'technique_')\n",
    "    mat = helpers.col_collection(data, 'material_')\n",
    "    size = data.columns[data.columns.str.contains('IN')]\n",
    "\n",
    "    features = [tech,mat,size]\n",
    "\n",
    "    for feat in features:\n",
    "        frequencies = {}\n",
    "        for col in feat:\n",
    "            frequencies[col] = data[col].sum()\n",
    "        frequencies = dict(sorted(frequencies.items(), key=lambda item: item[1], reverse=True))\n",
    "        instance_sum = 0\n",
    "        for col in frequencies:\n",
    "            frequency = frequencies[col]\n",
    "            #if instance_sum > threshold_sum or frequency < min_freq:\n",
    "            if frequency < min_freq:\n",
    "                data.drop(columns=[col], inplace=True)\n",
    "            instance_sum += frequency\n",
    "\n",
    "            \n",
    "    ## hot encoding & thresholding\n",
    "    # categorical columns\n",
    "    # already encoded\n",
    "    # material, technique, unit, size, value\n",
    "\n",
    "    cols = ['musealia_additional_nr', 'collection_mark', 'musealia_mark', 'museum_abbr', 'before_Christ', 'is_original', 'class', 'parish', 'state',  'event_type', 'participants_role', 'parish', 'color', 'collection_additional_nr', 'damages', 'participant', 'location', 'name', 'commentary', 'text', 'legend', 'initial_info', 'additional_text', 'country', 'city_municipality']\n",
    "\n",
    "    text_features = ['name', 'commentary', 'text', 'legend', 'initial_info', 'additional_text']\n",
    "    for col in cols:\n",
    "        data[col] = data[col].fillna('nan')\n",
    "        instance_sum = 0\n",
    "        val_counts = data[col].value_counts()\n",
    "        values_to_group = []\n",
    "        for idx, name in enumerate(val_counts.index):\n",
    "            frequency = val_counts[idx]\n",
    "            if instance_sum > threshold_sum or frequency < min_freq:\n",
    "                values_to_group.append(name)\n",
    "\n",
    "            instance_sum += frequency\n",
    "        data[col] = data[col].apply(lambda x: 'uncommon' if (x in values_to_group) else x)\n",
    "\n",
    "    # one hot encoding\n",
    "    data = pd.get_dummies(data, columns=cols)\n",
    "        \n",
    "    ## Delete unneeded features\n",
    "\n",
    "\n",
    "    data.drop(columns=['full_nr','country_and_unit','parameter','unit','value'], inplace=True)\n",
    "\n",
    "    ## continous numeric features (nan -> 0)\n",
    "    data = data.replace(np.nan, 0)\n",
    "    ## rename for xgboost (cant deal with <>[] in feature names)\n",
    "    for i in data.columns:\n",
    "        if '>' in i:\n",
    "            data.rename(columns={i:i.replace('>','')}, inplace=True)\n",
    "        if '<' in i:\n",
    "            data.rename(columns={i:i.replace('<','')}, inplace=True)\n",
    "        if ']' in i:\n",
    "            data.rename(columns={i:i.replace(']','')}, inplace=True)\n",
    "        if '[' in i:\n",
    "            data.rename(columns={i:i.replace('[','')}, inplace=True)\n",
    "\n",
    "    # resplit test/train\n",
    "    train = data.loc[data['source']=='train'].drop('source',axis=1)\n",
    "\n",
    "    # modify types\n",
    "    train['type'] = train['type'].replace('fotonegatiiv, fotonegatiiv', 'fotonegatiiv')\n",
    "    \n",
    "\n",
    "    # resplit test/train\n",
    "    train, val = train_test_split(train, test_size=0.3, random_state=0)\n",
    "    test = data.loc[data['source']=='test'].drop('source',axis=1)\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "#function to have resamplers resample to specific number of samples per class\n",
    "def by_num(y, min_samples):\n",
    "    b = Counter(y).values()\n",
    "    a = Counter(y).keys()\n",
    "    a = list(a)\n",
    "    b = list(b)\n",
    "\n",
    "    if min_samples > max(b):\n",
    "        min_samples = max(b)\n",
    "\n",
    "    for i in range(len(a)):\n",
    "        if b[i] < min_samples :\n",
    "            b[i] = min_samples\n",
    "    return dict(zip(a, b))\n",
    "\n",
    "#function to have resamplers resample to specific number of samples per class\n",
    "def by_perc(y, increase_perc):\n",
    "    a = Counter(y).keys()\n",
    "    b = Counter(y).values()\n",
    "    a = list(a)\n",
    "    b = list(b)\n",
    "\n",
    "    max_samples = max(b)\n",
    "\n",
    "    for i in range(len(b)):\n",
    "        new_samples = int(b[i] * (1 + increase_perc/100))\n",
    "        if new_samples > max_samples:\n",
    "            b[i] = max_samples\n",
    "        else:\n",
    "            b[i] = new_samples\n",
    "    return dict(zip(a, b))\n",
    "\n",
    "\n",
    "def rebalancing(X, y, reb_method, strategy, by_value):\n",
    "\n",
    "    if strategy == 'perc':\n",
    "        sampling_strategy = by_perc\n",
    "    else:\n",
    "        sampling_strategy = by_num\n",
    "    \n",
    "    if reb_method == 'smote':\n",
    "        balancer = SMOTE(sampling_strategy=sampling_strategy(y,by_value), random_state=0)\n",
    "    elif reb_method == 'ros':\n",
    "        balancer = RandomOverSampler(sampling_strategy=sampling_strategy(y,by_value), random_state=0)\n",
    "    else:\n",
    "        return X, y\n",
    "\n",
    "    X_res, y_res = balancer.fit_resample(X, y)\n",
    "\n",
    "    return X_res, y_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data prep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10821/659318557.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['startYear'].iloc[i] = data['endYear'].iloc[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 columns found that start with technique_\n",
      "111 columns found that start with material_\n",
      "balancing\n",
      "('num', 100)\n",
      "num 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\nX_val = val.drop(\\'type\\', axis=1)\\ny_val = val.type\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # note that we define values from `wandb.config` instead \n",
    "# of defining hard values \n",
    "min_samples_split = 5\n",
    "max_depth = 100\n",
    "min_samples_leaf = 1\n",
    "n_estimators = 100\n",
    "max_features = 'sqrt'\n",
    "criterion = 'gini'\n",
    "feat_percent_cut = 84\n",
    "feat_freq_cut = 15\n",
    "reb_method = 'ros'\n",
    "rebalance = ('num',100)\n",
    "class_weight = None\n",
    "\n",
    "# -------------------------- data prep code  -------------------------------------\n",
    "\n",
    "print('data prep')\n",
    "train, val, test = get_data(feat_percent_cut=feat_percent_cut, feat_freq_cut=feat_freq_cut)\n",
    "\n",
    "print('balancing')\n",
    "print(rebalance)\n",
    "strategy, by_value = rebalance\n",
    "print(strategy, by_value)\n",
    "\n",
    "#val = val_est_prepared.copy()\n",
    "\n",
    "X_train = train.drop('type', axis=1)\n",
    "y_train = train.type\n",
    "\n",
    "\"\"\"\"\n",
    "X_val = val.drop('type', axis=1)\n",
    "y_val = val.type\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(y_train)\n",
    "\n",
    "y_train = label_encoder.transform(y_train)\n",
    "#y_val = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "crossval_acc 0.8860204081632652\n",
      "crossval_f1_macro 0.6292118932638248\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------- usual training code starts here  -------------------------------------\n",
    "print('training')\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_leaf=min_samples_leaf,\\\n",
    "        max_features=max_features, min_samples_split=min_samples_split, class_weight=class_weight, random_state=0)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "\n",
    "val_acc = []\n",
    "val_f1_macro = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n",
    "    print('fold', i)\n",
    "    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    # replace uncommon types\n",
    "    unique, counts = np.unique(y_train_fold, return_counts=True)\n",
    "    # 6 to have 5 samples per class left for standard knn in smote\n",
    "    # -> uncommon classes become 100\n",
    "    for i in np.argwhere(counts < 6):\n",
    "        y_train_fold[y_train_fold == i[0]] = 100\n",
    "\n",
    "    X_train_fold, y_train_fold = rebalancing(X_train_fold, y_train_fold, reb_method=reb_method, strategy=strategy, by_value=by_value)\n",
    "\n",
    "    rfc.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    y_pred = rfc.predict(X_test_fold)\n",
    "    val_acc.append(accuracy_score(y_test_fold, y_pred))\n",
    "    val_f1_macro.append(f1_score(y_test_fold, y_pred, average='macro'))\n",
    "\n",
    "crossval_acc = np.mean(val_acc)\n",
    "crossval_f1_macro = np.mean(val_f1_macro)\n",
    "print('crossval_acc', crossval_acc)\n",
    "print('crossval_f1_macro', crossval_f1_macro)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
